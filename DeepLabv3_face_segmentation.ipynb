{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b6dfdf",
   "metadata": {
    "papermill": {
     "duration": 0.004195,
     "end_time": "2025-02-25T19:41:06.854250",
     "exception": false,
     "start_time": "2025-02-25T19:41:06.850055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SLICE MY FACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4330b",
   "metadata": {
    "papermill": {
     "duration": 0.003262,
     "end_time": "2025-02-25T19:41:06.861348",
     "exception": false,
     "start_time": "2025-02-25T19:41:06.858086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook, I trained a **DeepLabV3+** model for facial feature segmentation.  \n",
    "Trained the model on **3500 train images** and generated **RLE-encoded masks** for test images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6192ebfd",
   "metadata": {
    "papermill": {
     "duration": 0.00322,
     "end_time": "2025-02-25T19:41:06.868023",
     "exception": false,
     "start_time": "2025-02-25T19:41:06.864803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**IMPORTING THE REQUIRED LIBRARIES**\n",
    "1. pandas, numpy, os for file and data management\n",
    "2. PIL.Image, cv2 for handling and augmenting images\n",
    "3. Torch, torchvision.transforms for model training and preprocessing\n",
    "4. tqmd for progress bar\n",
    "5. DataLoader loads data in batches with shuffling and multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1cd59e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:41:06.875991Z",
     "iopub.status.busy": "2025-02-25T19:41:06.875695Z",
     "iopub.status.idle": "2025-02-25T19:41:15.360999Z",
     "shell.execute_reply": "2025-02-25T19:41:15.360070Z"
    },
    "papermill": {
     "duration": 8.491272,
     "end_time": "2025-02-25T19:41:15.362620",
     "exception": false,
     "start_time": "2025-02-25T19:41:06.871348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac03ca84",
   "metadata": {
    "papermill": {
     "duration": 0.003256,
     "end_time": "2025-02-25T19:41:15.369770",
     "exception": false,
     "start_time": "2025-02-25T19:41:15.366514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Defining the path of the datasets**\n",
    "\n",
    "Dataset path is stored in DATASET_PATH.\n",
    "The Dataset is divided into two parts- annotations and images, which are further divided into train,test and val.\n",
    "TRAIN_IMG_DIR contains the images and TRAIN_MASK_DIR contains the masks on which the model is trained, then the model predicts the mask values for the images stored in TEST_IMG_DIR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1930e504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:41:15.377705Z",
     "iopub.status.busy": "2025-02-25T19:41:15.377304Z",
     "iopub.status.idle": "2025-02-25T19:41:15.381070Z",
     "shell.execute_reply": "2025-02-25T19:41:15.380280Z"
    },
    "papermill": {
     "duration": 0.009083,
     "end_time": "2025-02-25T19:41:15.382215",
     "exception": false,
     "start_time": "2025-02-25T19:41:15.373132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/kaggle/input/slicee-my-face\"\n",
    "TRAIN_IMG_DIR = os.path.join(DATASET_PATH, \"images/train\")\n",
    "TRAIN_MASK_DIR = os.path.join(DATASET_PATH, \"annotations/train\")\n",
    "TEST_IMG_DIR = os.path.join(DATASET_PATH, \"images/test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d11566",
   "metadata": {
    "papermill": {
     "duration": 0.003394,
     "end_time": "2025-02-25T19:41:15.389041",
     "exception": false,
     "start_time": "2025-02-25T19:41:15.385647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "IMG_SIZE = (256,256): Standardizes all images and masks to 256×256 pixels for uniformity.\n",
    "\n",
    "CUDA Support: Detects if a GPU is available and assigns the device.\n",
    "\n",
    "Defined the batch_size=8(all the 3500 are divided into 8 batches for training),epochs=5(number of iterations) and learning rate=0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fea13a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:41:15.396907Z",
     "iopub.status.busy": "2025-02-25T19:41:15.396662Z",
     "iopub.status.idle": "2025-02-25T19:41:15.450192Z",
     "shell.execute_reply": "2025-02-25T19:41:15.449265Z"
    },
    "papermill": {
     "duration": 0.059057,
     "end_time": "2025-02-25T19:41:15.451569",
     "exception": false,
     "start_time": "2025-02-25T19:41:15.392512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (256, 256)  # Resize images\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 1e-4  # Learning rate\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9cce7",
   "metadata": {
    "papermill": {
     "duration": 0.003188,
     "end_time": "2025-02-25T19:41:15.458425",
     "exception": false,
     "start_time": "2025-02-25T19:41:15.455237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom Dataset Class for Facial Segmentation\n",
    "\n",
    "This class loads the images and masks for training.  \n",
    "For test images, only images are loaded.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590f543d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:41:15.466246Z",
     "iopub.status.busy": "2025-02-25T19:41:15.465958Z",
     "iopub.status.idle": "2025-02-25T19:41:15.472387Z",
     "shell.execute_reply": "2025-02-25T19:41:15.471609Z"
    },
    "papermill": {
     "duration": 0.011879,
     "end_time": "2025-02-25T19:41:15.473740",
     "exception": false,
     "start_time": "2025-02-25T19:41:15.461861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir=None, transform=None, train=True):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = sorted(os.listdir(img_dir))\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.train:\n",
    "            # Load mask if in training mode\n",
    "            mask_path = os.path.join(self.mask_dir, img_name.replace(\".jpg\", \".png\"))\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = (mask > 0).astype(np.uint8)  # Ensure binary mask\n",
    "\n",
    "            # Apply transformations\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=image, mask=mask)\n",
    "                image, mask = transformed[\"image\"], transformed[\"mask\"]\n",
    "            \n",
    "            return image, mask\n",
    "        \n",
    "        else:\n",
    "            # For test images, return only image\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)[\"image\"]\n",
    "            return image, img_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93114f11",
   "metadata": {
    "papermill": {
     "duration": 0.003512,
     "end_time": "2025-02-25T19:41:15.480732",
     "exception": false,
     "start_time": "2025-02-25T19:41:15.477220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Transformations using Albumentations\n",
    "Resized the images, normalized them, and applied basic augmentations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858ebe45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:41:15.488189Z",
     "iopub.status.busy": "2025-02-25T19:41:15.487973Z",
     "iopub.status.idle": "2025-02-25T19:41:17.076483Z",
     "shell.execute_reply": "2025-02-25T19:41:17.075347Z"
    },
    "papermill": {
     "duration": 1.594068,
     "end_time": "2025-02-25T19:41:17.078071",
     "exception": false,
     "start_time": "2025-02-25T19:41:15.484003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6642ae",
   "metadata": {
    "papermill": {
     "duration": 0.003411,
     "end_time": "2025-02-25T19:41:17.085441",
     "exception": false,
     "start_time": "2025-02-25T19:41:17.082030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading the Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d307dee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:41:17.093755Z",
     "iopub.status.busy": "2025-02-25T19:41:17.093308Z",
     "iopub.status.idle": "2025-02-25T19:41:17.433611Z",
     "shell.execute_reply": "2025-02-25T19:41:17.432618Z"
    },
    "papermill": {
     "duration": 0.346324,
     "end_time": "2025-02-25T19:41:17.435318",
     "exception": false,
     "start_time": "2025-02-25T19:41:17.088994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FaceSegmentationDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, transform=train_transform, train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = FaceSegmentationDataset(TEST_IMG_DIR, transform=test_transform, train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb5a61",
   "metadata": {
    "papermill": {
     "duration": 0.003446,
     "end_time": "2025-02-25T19:41:17.442732",
     "exception": false,
     "start_time": "2025-02-25T19:41:17.439286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DeepLabV3+ Model\n",
    "Used DeepLabV3+ with a modified classifier for binary segmentation and **Dice Loss + BCE Loss** for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "513ffafc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:41:17.450647Z",
     "iopub.status.busy": "2025-02-25T19:41:17.450319Z",
     "iopub.status.idle": "2025-02-25T19:41:20.543611Z",
     "shell.execute_reply": "2025-02-25T19:41:20.542825Z"
    },
    "papermill": {
     "duration": 3.099054,
     "end_time": "2025-02-25T19:41:20.545195",
     "exception": false,
     "start_time": "2025-02-25T19:41:17.446141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n",
      "100%|██████████| 161M/161M [00:02<00:00, 83.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "class DeepLabV3Plus(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "        self.model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)  # Modify output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[\"out\"]\n",
    "\n",
    "# Initialize model\n",
    "model = DeepLabV3Plus().to(DEVICE)\n",
    "\n",
    "# Loss & Optimizer\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * target).sum()\n",
    "    return 1 - (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "\n",
    "criterion = lambda pred, target: 0.5 * dice_loss(pred, target) + 0.5 * nn.BCEWithLogitsLoss()(pred, target)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949fec77",
   "metadata": {
    "papermill": {
     "duration": 0.005217,
     "end_time": "2025-02-25T19:41:20.555730",
     "exception": false,
     "start_time": "2025-02-25T19:41:20.550513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Loop\n",
    "Trained for 5 epochs and printed the loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956e5259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:41:20.565909Z",
     "iopub.status.busy": "2025-02-25T19:41:20.565639Z",
     "iopub.status.idle": "2025-02-25T20:08:00.404242Z",
     "shell.execute_reply": "2025-02-25T20:08:00.403179Z"
    },
    "papermill": {
     "duration": 1599.845319,
     "end_time": "2025-02-25T20:08:00.405709",
     "exception": false,
     "start_time": "2025-02-25T19:41:20.560390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 438/438 [06:06<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 438/438 [05:09<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 438/438 [05:09<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.0314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 438/438 [05:07<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 438/438 [05:06<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.0237\n",
      "Model saved! ✅\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE).float().unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), \"deeplabv3_model.pth\")\n",
    "print(\"Model saved! ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40be3b6",
   "metadata": {
    "papermill": {
     "duration": 0.09974,
     "end_time": "2025-02-25T20:08:00.606870",
     "exception": false,
     "start_time": "2025-02-25T20:08:00.507130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Code to convert the mask into rle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c40303e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:08:00.829468Z",
     "iopub.status.busy": "2025-02-25T20:08:00.829001Z",
     "iopub.status.idle": "2025-02-25T20:08:00.836828Z",
     "shell.execute_reply": "2025-02-25T20:08:00.835889Z"
    },
    "papermill": {
     "duration": 0.116883,
     "end_time": "2025-02-25T20:08:00.838548",
     "exception": false,
     "start_time": "2025-02-25T20:08:00.721665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    \"\"\"Convert binary mask to RLE format.\"\"\"\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])  # Add padding\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56fb34",
   "metadata": {
    "papermill": {
     "duration": 0.098804,
     "end_time": "2025-02-25T20:08:01.066861",
     "exception": false,
     "start_time": "2025-02-25T20:08:00.968057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate Predictions and Save Submission File\n",
    "Predicted masks for test images and encode them using RLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536c6101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:08:01.269612Z",
     "iopub.status.busy": "2025-02-25T20:08:01.269294Z",
     "iopub.status.idle": "2025-02-25T20:09:41.983457Z",
     "shell.execute_reply": "2025-02-25T20:09:41.982715Z"
    },
    "papermill": {
     "duration": 100.818954,
     "end_time": "2025-02-25T20:09:41.984822",
     "exception": false,
     "start_time": "2025-02-25T20:08:01.165868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 1000/1000 [01:40<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as submission.csv ✅\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "submission = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, img_name in tqdm(test_loader, desc=\"Generating Predictions\"):\n",
    "        image = image.to(DEVICE)\n",
    "\n",
    "        # Predict mask\n",
    "        output = model(image)\n",
    "        pred_mask = torch.sigmoid(output).cpu().numpy().squeeze()\n",
    "\n",
    "        # Convert to binary mask (threshold = 0.5)\n",
    "        binary_mask = (pred_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Resize back to original dimensions\n",
    "        original_size = cv2.imread(os.path.join(TEST_IMG_DIR, img_name[0])).shape[:2]\n",
    "        binary_mask = cv2.resize(binary_mask, (original_size[1], original_size[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Encode as RLE\n",
    "        rle_mask = rle_encode(binary_mask)\n",
    "        submission.append([img_name[0].split(\".\")[0], rle_mask])\n",
    "\n",
    "# Save to CSV\n",
    "submission_df = pd.DataFrame(submission, columns=[\"id\", \"predicted\"])\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission file saved as submission.csv ✅\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11074257,
     "sourceId": 92860,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1720.611094,
   "end_time": "2025-02-25T20:09:44.782842",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-25T19:41:04.171748",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
